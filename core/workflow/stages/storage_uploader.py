# src/pipeline/stages/storage_stage.py
"""ìŠ¤í† ë¦¬ì§€ ì—…ë¡œë“œ ìŠ¤í…Œì´ì§€ - ë°°ì¹˜ ì—…ë¡œë“œ ì§€ì›"""

import os
from integrations.storage.interface import StorageType, StorageManager

from ..pipeline import PipelineStage, PipelineContext


class StorageUploadStage(PipelineStage):
    """ìŠ¤í† ë¦¬ì§€ ì—…ë¡œë“œ"""
    
    def __init__(self, storage_type: StorageType = None):
        super().__init__("storage_upload")
        
        # ìŠ¤í† ë¦¬ì§€ íƒ€ì… ê²°ì •
        if storage_type is None:
            storage_type_str = os.getenv("STORAGE_TYPE", "local").lower()
            storage_type = StorageType.SFTP if storage_type_str == "sftp" else StorageType.LOCAL
        
        self.storage_manager = StorageManager(storage_type)
        self.storage_type = storage_type
    
    def can_skip(self, context: PipelineContext) -> bool:
        """ë¡œì»¬ ìŠ¤í† ë¦¬ì§€ì´ê±°ë‚˜ ìºì‹œ íˆíŠ¸ ì‹œ ìŠ¤í‚µ"""
        # í™˜ê²½ë³€ìˆ˜ë¡œ ë¡œì»¬ ìŠ¤í† ë¦¬ì§€ ìŠ¤í‚µ ì—¬ë¶€ ê²°ì •
        skip_local_storage = os.getenv("SKIP_LOCAL_STORAGE", "true").lower() == "true"
        
        return ((self.storage_type == StorageType.LOCAL and skip_local_storage) or 
                context.stage_results.get("cache_hit", False))
    
    def execute(self, context: PipelineContext) -> PipelineContext:
        """ìŠ¤í† ë¦¬ì§€ ì—…ë¡œë“œ ì‹¤í–‰"""
        self.update_progress(85, "ğŸ“¤ ìŠ¤í† ë¦¬ì§€ ì—…ë¡œë“œ ì‹œì‘...", context)
        
        video = context.video_object
        remote_base_path = f"video_analysis/{video.session_id}"
        
        # ì„¸ì…˜ ë””ë ‰í† ë¦¬ í™•ì¸
        self.logger.info(f"ğŸ“ ì„¸ì…˜ ë””ë ‰í† ë¦¬: {video.session_dir}")
        
        # ëª¨ë“  ì—…ë¡œë“œí•  íŒŒì¼ì„ ìˆ˜ì§‘
        file_pairs = []
        
        # 1. ë¹„ë””ì˜¤ íŒŒì¼
        if video.local_path and os.path.exists(video.local_path):
            video_filename = os.path.basename(video.local_path)
            remote_video_path = os.path.join(remote_base_path, video_filename)
            file_pairs.append((video.local_path, remote_video_path))
            self.logger.info(f"ğŸ“¹ ë¹„ë””ì˜¤ íŒŒì¼ ì¶”ê°€: {video_filename}")
        
        # 2. ì¸ë„¤ì¼
        thumbnail_path = os.path.join(video.session_dir, f"{video.session_id}_Thumbnail.jpg")
        if os.path.exists(thumbnail_path):
            remote_thumb_path = os.path.join(remote_base_path, f"{video.session_id}_Thumbnail.jpg")
            file_pairs.append((thumbnail_path, remote_thumb_path))
            self.logger.info(f"ğŸ–¼ï¸ ì¸ë„¤ì¼ íŒŒì¼ ì¶”ê°€")
        
        # 3. ì”¬ ì´ë¯¸ì§€ (scenes ë””ë ‰í† ë¦¬)
        scenes_dir = os.path.join(video.session_dir, "scenes")
        if os.path.exists(scenes_dir):
            scene_files = sorted([f for f in os.listdir(scenes_dir) if f.endswith('.jpg')])
            self.logger.info(f"ğŸ¬ ì”¬ ì´ë¯¸ì§€ ì¶”ê°€: {len(scene_files)}ê°œ")
            
            for scene_file in scene_files:
                scene_path = os.path.join(scenes_dir, scene_file)
                remote_scene_path = os.path.join(remote_base_path, scene_file)
                file_pairs.append((scene_path, remote_scene_path))
        
        # 4. ê·¸ë£¹í™”ëœ ì´ë¯¸ì§€ (grouped ë””ë ‰í† ë¦¬)
        grouped_dir = os.path.join(video.session_dir, "grouped")
        if os.path.exists(grouped_dir):
            grouped_files = sorted([f for f in os.listdir(grouped_dir) if f.endswith('.jpg')])
            self.logger.info(f"ğŸ¨ ê·¸ë£¹ ì´ë¯¸ì§€ ì¶”ê°€: {len(grouped_files)}ê°œ")
            
            for grouped_file in grouped_files:
                grouped_path = os.path.join(grouped_dir, grouped_file)
                remote_grouped_path = os.path.join(remote_base_path, grouped_file)
                file_pairs.append((grouped_path, remote_grouped_path))
        
        # 5. ë¶„ì„ ê²°ê³¼
        if video.analysis_result:
            analysis_path = os.path.join(video.session_dir, "analysis_result.json")
            
            if not os.path.exists(analysis_path):
                import json
                os.makedirs(os.path.dirname(analysis_path), exist_ok=True)
                with open(analysis_path, 'w', encoding='utf-8') as f:
                    json.dump(video.analysis_result, f, ensure_ascii=False, indent=2)
            
            remote_analysis_path = os.path.join(remote_base_path, "analysis_result.json")
            file_pairs.append((analysis_path, remote_analysis_path))
            self.logger.info(f"ğŸ“„ ë¶„ì„ ê²°ê³¼ íŒŒì¼ ì¶”ê°€")
        
        # íŒŒì¼ ê°œìˆ˜ í†µê³„
        self.logger.info(f"ğŸ“Š ì—…ë¡œë“œí•  íŒŒì¼ ì´ {len(file_pairs)}ê°œ:")
        self.logger.info(f"  - ë¹„ë””ì˜¤: {sum(1 for p in file_pairs if 'mp4' in p[0] or 'webm' in p[0])}ê°œ")
        self.logger.info(f"  - ì´ë¯¸ì§€: {sum(1 for p in file_pairs if '.jpg' in p[0])}ê°œ")
        self.logger.info(f"  - JSON: {sum(1 for p in file_pairs if '.json' in p[0])}ê°œ")
        
        # ë°°ì¹˜ ì—…ë¡œë“œ ì‹¤í–‰
        if self.storage_type == StorageType.SFTP and len(file_pairs) > 1:
            # SFTPì˜ ê²½ìš° ë°°ì¹˜ ì—…ë¡œë“œ ì‚¬ìš©
            self.logger.info(f"ğŸš€ SFTP ë°°ì¹˜ ì—…ë¡œë“œ ì‹œì‘ (ë™ì‹œ {os.getenv('SFTP_MAX_CONCURRENT', '5')}ê°œ ì—°ê²°)")
            # ì§„í–‰ë¥  ì½œë°± ì—†ì´ ì‹¤í–‰ (í˜„ì¬ ë²„ê·¸ê°€ ìˆìŒ)
            results = self.storage_manager.upload_files_batch(file_pairs)
            
            # ê²°ê³¼ ë¶„ì„
            uploaded = sum(1 for r in results if r["success"])
            failed = [r for r in results if not r["success"]]
            
            self.logger.info(f"ğŸ“Š ì—…ë¡œë“œ ì™„ë£Œ: {uploaded}/{len(file_pairs)}ê°œ íŒŒì¼")
            
            if failed:
                self.logger.warning(f"âš ï¸ ì‹¤íŒ¨í•œ íŒŒì¼ {len(failed)}ê°œ:")
                for f in failed:
                    self.logger.error(f"  âŒ {f['local_path']}: {f['error']}")
        else:
            # ë¡œì»¬ ìŠ¤í† ë¦¬ì§€ë‚˜ íŒŒì¼ì´ 1ê°œì¸ ê²½ìš° ìˆœì°¨ ì²˜ë¦¬
            self.logger.info(f"ğŸ“¤ ìˆœì°¨ ì—…ë¡œë“œ ì‹œì‘")
            uploaded = 0
            
            for i, (local_path, remote_path) in enumerate(file_pairs):
                filename = os.path.basename(local_path)
                self.logger.info(f"ğŸ“¤ ì—…ë¡œë“œ [{i+1}/{len(file_pairs)}]: {filename}")
                
                if self.storage_manager.upload_file(local_path, remote_path):
                    uploaded += 1
                    self.logger.info(f"âœ… {filename} ì—…ë¡œë“œ ì„±ê³µ")
                else:
                    self.logger.error(f"âŒ {filename} ì—…ë¡œë“œ ì‹¤íŒ¨")
                
                # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
                progress = 85 + int(((i+1) / len(file_pairs)) * 10)
                self.update_progress(progress, f"ğŸ“¤ ì—…ë¡œë“œ ì¤‘: {filename}", context)
            
            self.logger.info(f"ğŸ“Š ì—…ë¡œë“œ ì™„ë£Œ: {uploaded}/{len(file_pairs)}ê°œ íŒŒì¼")
        
        self.update_progress(95, "âœ… ìŠ¤í† ë¦¬ì§€ ì—…ë¡œë“œ ì™„ë£Œ", context)
        
        return context
