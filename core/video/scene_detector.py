# src/extractor/scene_extractor.py
"""ì •ë°€ë„ ë ˆë²¨ ê¸°ë°˜ ê°œì„ ëœ ì”¬ ì¶”ì¶œ ë° ê·¸ë£¹í™” ëª¨ë“ˆ"""

import os
import cv2
import numpy as np
from typing import List, Dict, Any, Tuple, Optional, Callable
from pathlib import Path
import subprocess
import json
from sklearn.cluster import DBSCAN
from sklearn.preprocessing import StandardScaler
from scipy.spatial.distance import cdist
import imagehash
from PIL import Image
from collections import defaultdict
from utils.logger import get_logger
from core.video.models import Scene

logger = get_logger(__name__)



class SceneExtractor:
    """ì˜ìƒì—ì„œ ì£¼ìš” ì”¬ì„ ì¶”ì¶œí•˜ê³  ì •ë°€ë„ ë ˆë²¨ì— ë”°ë¼ ì •êµí•˜ê²Œ ê·¸ë£¹í™”"""
    
    def __init__(self):
        self.logger = get_logger(__name__)
        self.temp_dir = "data/temp"
        
        # ê¸°ë³¸ê°’ ì„¤ì • (load_settings ì „ì— ì´ˆê¸°í™”)
        self.precision_level = 5
        self.target_scene_count = 6  # ê¸°ë³¸ê°’ ì„¤ì •
        
        # ì´ˆê¸° ì„¤ì • ë¡œë“œ
        self.load_settings()
        
        # ì •ë°€ë„ ë ˆë²¨ë³„ íŠ¹ì§• ê°€ì¤‘ì¹˜ ë° ëª©í‘œ ì”¬ ê°œìˆ˜ ì„¤ì •
        self._setup_precision_weights()
        
        self.logger.info(f"ğŸ¯ SceneExtractor ì´ˆê¸°í™” - ì •ë°€ë„ ë ˆë²¨: {self.precision_level}")
    
    def load_settings(self):
        """í™˜ê²½ë³€ìˆ˜ì—ì„œ ì„¤ì • ë¡œë“œ"""
        # ì •ë°€ë„ ë ˆë²¨ (1-10) - ë”°ì˜´í‘œ ì²˜ë¦¬ ì¶”ê°€
        precision_str = os.getenv("SCENE_PRECISION_LEVEL")
        # ë”°ì˜´í‘œ ì œê±°
        precision_str = precision_str.strip("'\"")
        
        try:
            self.precision_level = int(precision_str)
        except ValueError:
            self.logger.warning(f"ì˜ëª»ëœ SCENE_PRECISION_LEVEL ê°’: {precision_str}, ê¸°ë³¸ê°’ 5 ì‚¬ìš©")
            self.precision_level = 5
        
        # ì”¬ ì¶”ì¶œ ì„¤ì •
        self.scene_threshold = float(os.getenv("SCENE_THRESHOLD", "0.3"))
        self.min_scene_duration = float(os.getenv("MIN_SCENE_DURATION", "0.5"))
        
        # ê·¸ë£¹í™” ì„¤ì •
        self.min_scenes_for_grouping = int(os.getenv("MIN_SCENES_FOR_GROUPING", "10"))
        self.similarity_threshold = float(os.getenv("SCENE_SIMILARITY_THRESHOLD", "0.92"))
        self.hash_threshold = int(os.getenv("SCENE_HASH_THRESHOLD", "5"))
        self.max_output_scenes = int(os.getenv("MAX_ANALYSIS_IMAGES", "10"))
        
        # ë¡œê·¸ ì¶”ê°€í•˜ì—¬ ì‹¤ì œ ë¡œë“œëœ ê°’ í™•ì¸
        self.logger.info(f"ğŸ“‹ ì„¤ì • ë¡œë“œ ì™„ë£Œ:")
        self.logger.info(f"  - SCENE_PRECISION_LEVEL: {self.precision_level}")
        # target_scene_countëŠ” _setup_precision_weights ì´í›„ì— ì„¤ì •ë˜ë¯€ë¡œ ì—¬ê¸°ì„œëŠ” ë¡œê·¸í•˜ì§€ ì•ŠìŒ
        
    def update_settings(self):
        """í™˜ê²½ë³€ìˆ˜ì—ì„œ ìµœì‹  ì„¤ì •ì„ ë‹¤ì‹œ ì½ì–´ì˜´"""
        # ì´ì „ ì„¤ì • ì €ì¥
        old_precision_level = self.precision_level
        old_scene_threshold = self.scene_threshold
        old_similarity_threshold = self.similarity_threshold
        
        # ìƒˆë¡œìš´ ì„¤ì • ë¡œë“œ
        self.load_settings()
        
        # ë³€ê²½ì‚¬í•­ ë¡œê¹…
        settings_changed = False
        
        if old_precision_level != self.precision_level:
            self.logger.info(f"ğŸ”„ ì •ë°€ë„ ë ˆë²¨ ë³€ê²½: {old_precision_level} â†’ {self.precision_level}")
            settings_changed = True
        
        if abs(old_scene_threshold - self.scene_threshold) > 0.001:
            self.logger.info(f"ğŸ”„ ì”¬ ì „í™˜ ì„ê³„ê°’ ë³€ê²½: {old_scene_threshold:.3f} â†’ {self.scene_threshold:.3f}")
            settings_changed = True
        
        if abs(old_similarity_threshold - self.similarity_threshold) > 0.001:
            self.logger.info(f"ğŸ”„ ì”¬ ìœ ì‚¬ë„ ì„ê³„ê°’ ë³€ê²½: {old_similarity_threshold:.3f} â†’ {self.similarity_threshold:.3f}")
            settings_changed = True
        
        # ì •ë°€ë„ ë ˆë²¨ì´ ë³€ê²½ë˜ì—ˆìœ¼ë©´ ê°€ì¤‘ì¹˜ ì¬ì„¤ì •
        if old_precision_level != self.precision_level:
            self._setup_precision_weights()
            self.logger.info(f"âœ… ì •ë°€ë„ ë ˆë²¨ {self.precision_level} ì„¤ì • ì ìš© ì™„ë£Œ")
        
        if not settings_changed:
            self.logger.debug("ì„¤ì • ë³€ê²½ì‚¬í•­ ì—†ìŒ")
        
        return settings_changed
    
    def _setup_precision_weights(self):
        """ì •ë°€ë„ ë ˆë²¨ì— ë”°ë¥¸ íŠ¹ì§• ê°€ì¤‘ì¹˜ ë° ì¶œë ¥ ì„¤ì •"""
        
        # ê¸°ë³¸ ê°€ì¤‘ì¹˜ (ë ˆë²¨ 10)
        base_weights = {
            'color_hist': 0.25,      # ìƒ‰ìƒ íˆìŠ¤í† ê·¸ë¨
            'edge_density': 0.20,    # ì—£ì§€ ë°€ë„
            'texture': 0.20,         # í…ìŠ¤ì²˜ íŠ¹ì§•
            'spatial_color': 0.15,   # ê³µê°„ë³„ ìƒ‰ìƒ ë¶„í¬
            'perceptual_hash': 0.10, # ì§€ê°ì  í•´ì‹œ
            'brightness': 0.05,      # ë°ê¸°
            'contrast': 0.03,        # ëŒ€ë¹„
            'color_diversity': 0.02  # ìƒ‰ìƒ ë‹¤ì–‘ì„±
        }
        
        # ì •ë°€ë„ ë ˆë²¨ë³„ ì„¤ì •
        if self.precision_level == 1:
            # ì´ˆê³ ì†: ìƒ‰ìƒ íˆìŠ¤í† ê·¸ë¨ë§Œ
            self.feature_weights = {'color_hist': 1.0}
            self.active_features = ['color_hist']
            self.target_scene_count = 4  # ìµœì†Œí•œì˜ ë‹¤ì–‘ì„±
            
        elif self.precision_level == 2:
            # ê³ ì†: ìƒ‰ìƒ + ê¸°ë³¸ ì—£ì§€
            self.feature_weights = {'color_hist': 0.7, 'edge_density': 0.3}
            self.active_features = ['color_hist', 'edge_density']
            self.target_scene_count = 4
            
        elif self.precision_level == 3:
            # ë¹ ë¦„: ìƒ‰ìƒ + ì—£ì§€ + ë°ê¸°
            self.feature_weights = {'color_hist': 0.6, 'edge_density': 0.25, 'brightness': 0.15}
            self.active_features = ['color_hist', 'edge_density', 'brightness']
            self.target_scene_count = 5
            
        elif self.precision_level == 4:
            # ë³´í†µ-ë¹ ë¦„: ê¸°ë³¸ íŠ¹ì§•ë“¤
            self.feature_weights = {
                'color_hist': 0.4, 'edge_density': 0.3, 
                'brightness': 0.2, 'contrast': 0.1
            }
            self.active_features = ['color_hist', 'edge_density', 'brightness', 'contrast']
            self.target_scene_count = 5
            
        elif self.precision_level == 5:
            # ê· í˜•: ê¶Œì¥ ì„¤ì • - 6ê°œ ëª©í‘œ
            self.feature_weights = {
                'color_hist': 0.35, 'edge_density': 0.25, 
                'brightness': 0.15, 'contrast': 0.15, 'color_diversity': 0.1
            }
            self.active_features = ['color_hist', 'edge_density', 'brightness', 'contrast', 'color_diversity']
            self.target_scene_count = 6  # 6ê°œ ëª©í‘œ
            
        elif self.precision_level == 6:
            # ì •ë°€: ê¸°ë³¸ + í…ìŠ¤ì²˜ - 7ê°œ ëª©í‘œ
            self.feature_weights = {
                'color_hist': 0.3, 'edge_density': 0.2, 'texture': 0.2,
                'brightness': 0.1, 'contrast': 0.1, 'color_diversity': 0.1
            }
            self.active_features = ['color_hist', 'edge_density', 'texture', 'brightness', 'contrast', 'color_diversity']
            self.target_scene_count = 7
            
        elif self.precision_level == 7:
            # ê³ ì •ë°€: ê¸°ë³¸ + í…ìŠ¤ì²˜ + ê³µê°„ìƒ‰ìƒ - 8ê°œ ëª©í‘œ
            self.feature_weights = {
                'color_hist': 0.25, 'edge_density': 0.2, 'texture': 0.2,
                'spatial_color': 0.15, 'brightness': 0.08, 'contrast': 0.07, 'color_diversity': 0.05
            }
            self.active_features = ['color_hist', 'edge_density', 'texture', 'spatial_color', 'brightness', 'contrast', 'color_diversity']
            self.target_scene_count = 8
            
        elif self.precision_level == 8:
            # ë§¤ìš°ì •ë°€: ëŒ€ë¶€ë¶„ íŠ¹ì§• í™œì„±í™” - 10ê°œ ëª©í‘œ
            self.feature_weights = {
                'color_hist': 0.25, 'edge_density': 0.2, 'texture': 0.2,
                'spatial_color': 0.15, 'perceptual_hash': 0.08,
                'brightness': 0.05, 'contrast': 0.04, 'color_diversity': 0.03
            }
            self.active_features = ['color_hist', 'edge_density', 'texture', 'spatial_color', 'perceptual_hash', 'brightness', 'contrast', 'color_diversity']
            self.target_scene_count = 10
            
        elif self.precision_level == 9:
            # ì´ˆì •ë°€: ê±°ì˜ ëª¨ë“  íŠ¹ì§• - 10ê°œ ëª©í‘œ
            self.feature_weights = base_weights.copy()
            self.active_features = list(base_weights.keys())
            self.target_scene_count = 10
            
        else:  # ë ˆë²¨ 10
            # ìµœê³ ì •ë°€: ëª¨ë“  íŠ¹ì§• + ê³ í•´ìƒë„ - 10ê°œ ëª©í‘œ
            self.feature_weights = base_weights.copy()
            self.active_features = list(base_weights.keys())
            self.target_scene_count = 10
            # ê³ í•´ìƒë„ ì²˜ë¦¬ë¥¼ ìœ„í•œ ì¶”ê°€ ì„¤ì •
            self.high_resolution_mode = True
        
        # ê°€ì¤‘ì¹˜ ì •ê·œí™”
        total_weight = sum(self.feature_weights.values())
        if total_weight > 0:
            self.feature_weights = {k: v/total_weight for k, v in self.feature_weights.items()}
        
        self.logger.info(f"ğŸ”§ ì •ë°€ë„ ë ˆë²¨ {self.precision_level} ì„¤ì • ì™„ë£Œ")
        self.logger.info(f"ğŸ“‹ í™œì„±í™”ëœ íŠ¹ì§•: {', '.join(self.active_features)}")
        self.logger.info(f"ğŸ¯ ëª©í‘œ ì”¬ ê°œìˆ˜: {self.target_scene_count}ê°œ")
    
        
    def extract_scenes(self, video_path: str, session_id: str, progress_callback: Optional[Callable] = None) -> Dict[str, Any]:
        """ë¹„ë””ì˜¤ì—ì„œ ëª¨ë“  ì”¬ ì¶”ì¶œ í›„ ì •ë°€ë„ì— ë”°ë¼ ê·¸ë£¹í™”"""
        # ì‹œì‘í•˜ê¸° ì „ì— ìµœì‹  ì„¤ì • ë¡œë“œ
        settings_changed = self.update_settings()
        if settings_changed:
            self.logger.info("ğŸ”„ ë³€ê²½ëœ ì„¤ì •ìœ¼ë¡œ ì”¬ ì¶”ì¶œì„ ì‹œì‘í•©ë‹ˆë‹¤")
        
        try:
            output_dir = os.path.join(self.temp_dir, session_id, "scenes")
            os.makedirs(output_dir, exist_ok=True)
            
            self.logger.info(f"ğŸ¬ ì”¬ ì¶”ì¶œ ì‹œì‘")
            
            # 1. FFmpegë¡œ ëª¨ë“  ì”¬ ì „í™˜ì  ê²€ì¶œ (ì •ë°€ë„ì™€ ë¬´ê´€)
            self.logger.info("ğŸ” ì”¬ ì „í™˜ì  ê²€ì¶œ ì¤‘...")
            scene_changes = self._detect_scene_changes(video_path)
            
            if not scene_changes:
                self.logger.warning("ì”¬ ì „í™˜ì ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                return {'all_scenes': [], 'grouped_scenes': []}
            
            # 2. ë¹„ë””ì˜¤ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
            video_info = self._get_video_info(video_path)
            duration = float(video_info.get('duration', 0))
            width = int(video_info.get('width', 0))
            height = int(video_info.get('height', 0))
            
            # Shorts/Reels ê°ì§€
            is_short_form = duration <= 60 or (height > width and height/width > 1.5)
            if is_short_form:
                self.logger.info(f"ğŸ“± ì§§ì€ í˜•ì‹ ë™ì˜ìƒ ê°ì§€! (ê¸¸ì´: {duration:.1f}ì´ˆ, ë¹„ìœ¨: {width}x{height})")
                # ì§§ì€ ë™ì˜ìƒìš© ì„¤ì • ì¡°ì •
                self.min_scene_duration = 0.2  # ë” ì§§ì€ ì”¬ë„ í¬í•¨
                self.scene_threshold = 0.15  # ë” ë¯¼ê°í•œ ì”¬ ê°ì§€
            
            self.logger.info(f"ğŸ“¹ ì˜ìƒ ê¸¸ì´: {duration:.1f}ì´ˆ")
            
            # 3. ëª¨ë“  ì”¬ ì¤‘ê°„ì ì—ì„œ í”„ë ˆì„ ì¶”ì¶œ
            all_scenes = self._extract_frames_at_midpoints(
                video_path, scene_changes, output_dir, duration, progress_callback
            )
            
            self.logger.info(f"ğŸ“¸ ì´ {len(all_scenes)}ê°œ ì”¬ ì¶”ì¶œ ì™„ë£Œ")
            
            # 4. ì •ë°€ë„ ë ˆë²¨ì— ë”°ë¥¸ ê·¸ë£¹í™” ìˆ˜í–‰
            grouped_scenes = []
            if len(all_scenes) > 0:
                self.logger.info(f"ğŸ”¬ ì •ë°€ë„ ë ˆë²¨ {self.precision_level}ë¡œ ì”¬ ê·¸ë£¹í™” ì‹œì‘...")
                grouped_scenes = self._group_similar_scenes_precision(all_scenes.copy(), output_dir, progress_callback)
                
                # ê·¸ë£¹í™”ëœ ì”¬ë“¤ì„ ë³„ë„ ë””ë ‰í† ë¦¬ì— ì €ì¥
                grouped_scenes = self._save_grouped_scenes(grouped_scenes, session_id)
            
            self.logger.info(
                f"âœ… ì”¬ ì¶”ì¶œ ì™„ë£Œ - ì „ì²´: {len(all_scenes)}ê°œ, "
                f"ê·¸ë£¹í™”: {len(grouped_scenes)}ê°œ (ì •ë°€ë„ ë ˆë²¨: {self.precision_level})"
            )
            
            # 5. ê²°ê³¼ ë°˜í™˜ (ì „ì²´ ì”¬ê³¼ ê·¸ë£¹í™”ëœ ì”¬ ëª¨ë‘ í¬í•¨)
            return {
                'all_scenes': all_scenes,
                'grouped_scenes': grouped_scenes,
                'precision_level': self.precision_level,
                'target_count': self.target_scene_count
            }
            
        except Exception as e:
            self.logger.error(f"ì”¬ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return {'all_scenes': [], 'grouped_scenes': []}
        
    def _detect_scene_changes(self, video_path: str) -> List[float]:
        """FFmpegë¥¼ ì‚¬ìš©í•œ ëª¨ë“  ì”¬ ì „í™˜ì  ê²€ì¶œ (ì •ë°€ë„ì™€ ë¬´ê´€)"""
        # ì •ë°€ë„ ë ˆë²¨ ì¡°ì • ì œê±° - í•­ìƒ ë™ì¼í•œ ì„ê³„ê°’ ì‚¬ìš©
        threshold = self.scene_threshold
        
        cmd = [
            'ffmpeg',
            '-i', video_path,
            '-filter:v', f"select='gt(scene,{threshold})',showinfo",
            '-f', 'null',
            '-'
        ]
        
        try:
            result = subprocess.run(
                cmd,
                capture_output=True,
                text=True,
                check=False
            )
            
            # showinfo ì¶œë ¥ì—ì„œ íƒ€ì„ìŠ¤íƒ¬í”„ ì¶”ì¶œ
            timestamps = []
            for line in result.stderr.split('\n'):
                if 'pts_time:' in line:
                    try:
                        pts_time = float(line.split('pts_time:')[1].split()[0])
                        timestamps.append(pts_time)
                    except:
                        continue
            
            # ì‹œì‘ì  ì¶”ê°€
            if timestamps and timestamps[0] > 1.0:
                timestamps.insert(0, 0.0)
            elif not timestamps:
                timestamps = [0.0]
            
            self.logger.info(f"ğŸï¸ {len(timestamps)}ê°œ ì”¬ ì „í™˜ì  ê²€ì¶œ (ì„ê³„ê°’: {threshold:.3f})")
            
            return timestamps
            
        except Exception as e:
            self.logger.error(f"ì”¬ ê²€ì¶œ ì‹¤íŒ¨: {str(e)}")
            return []
    
    def _extract_frames_at_midpoints(
        self, 
        video_path: str, 
        scene_changes: List[float], 
        output_dir: str,
        duration: float,
        progress_callback: Optional[Callable] = None
    ) -> List[Scene]:
        """ì”¬ ì¤‘ê°„ì ì—ì„œ í”„ë ˆì„ ì¶”ì¶œ"""
        scenes = []
        
        # ë§ˆì§€ë§‰ íƒ€ì„ìŠ¤íƒ¬í”„ ì¶”ê°€
        if scene_changes[-1] < duration - 1:
            scene_changes.append(duration)
        
        # ì •ë°€ë„ ë ˆë²¨ì— ë”°ë¥¸ í’ˆì§ˆ ì„¤ì •
        quality = '2'  # ê¸°ë³¸ ê³ í’ˆì§ˆ
        if self.precision_level <= 3:
            quality = '5'  # ë¹ ë¥¸ ì²˜ë¦¬ë¥¼ ìœ„í•´ ë‚®ì€ í’ˆì§ˆ
        elif self.precision_level >= 8:
            quality = '1'  # ìµœê³  í’ˆì§ˆ
        
        # ê° ì”¬ì˜ ì¤‘ê°„ì  ê³„ì‚°
        total_scenes = len(scene_changes) - 1
        
        for i in range(len(scene_changes) - 1):
            start_time = scene_changes[i]
            end_time = scene_changes[i + 1]
            
            # ë„ˆë¬´ ì§§ì€ ì”¬ ì œì™¸
            if end_time - start_time < self.min_scene_duration:
                continue
            
            # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
            if progress_callback and total_scenes > 0:
                progress = int(40 + (i / total_scenes) * 30)  # 40-70% ë²”ìœ„
                progress_callback(progress, f"ğŸ“¸ í”„ë ˆì„ ì¶”ì¶œ ì¤‘... {i+1}/{total_scenes}")
            
            # ì¤‘ê°„ì  ê³„ì‚°
            mid_time = (start_time + end_time) / 2
            
            # í”„ë ˆì„ ì¶”ì¶œ
            output_path = os.path.join(output_dir, f"scene_{i:04d}.jpg")
            
            cmd = [
                'ffmpeg',
                '-ss', str(mid_time),
                '-i', video_path,
                '-frames:v', '1',
                '-q:v', quality,
                output_path,
                '-y'
            ]
            
            try:
                subprocess.run(cmd, capture_output=True, check=True)
                
                if os.path.exists(output_path):
                    scene = Scene(
                        timestamp=mid_time,
                        frame_path=output_path,
                        scene_type='mid'
                    )
                    scenes.append(scene)
                    
            except Exception as e:
                self.logger.error(f"í”„ë ˆì„ ì¶”ì¶œ ì‹¤íŒ¨ ({mid_time:.1f}ì´ˆ): {str(e)}")
        
        return scenes
    
    def _group_similar_scenes_precision(self, scenes: List[Scene], output_dir: str, progress_callback: Optional[Callable] = None) -> List[Scene]:
        """ì •ë°€ë„ ë ˆë²¨ ê¸°ë°˜ ì”¬ ê·¸ë£¹í™” ì•Œê³ ë¦¬ì¦˜ (ê°œì„ ëœ ë²„ì „)"""
        if len(scenes) <= self.min_scenes_for_grouping:
            # ì”¬ì´ ì ì€ ê²½ìš° ëª©í‘œ ê°œìˆ˜ì— ë§ì¶° ì¡°ì •
            if len(scenes) < self.target_scene_count:
                return scenes  # ëª¨ë“  ì”¬ ë°˜í™˜
            else:
                return self._select_diverse_scenes(scenes)[:self.target_scene_count]
        
        # 1. ì •ë°€ë„ ë ˆë²¨ì— ë”°ë¥¸ íŠ¹ì§• ì¶”ì¶œ
        self.logger.info(f"ğŸ”¬ ì •ë°€ë„ ë ˆë²¨ {self.precision_level} íŠ¹ì§• ì¶”ì¶œ ì¤‘...")
        features = []
        valid_scenes = []
        
        for i, scene in enumerate(scenes):
            try:
                if self.precision_level <= 5:
                    # ë¹ ë¥¸ ì²˜ë¦¬ë¥¼ ìœ„í•´ ì§„í–‰ë¥  ë¡œê¹… ìƒëµ
                    pass
                else:
                    # ìƒì„¸í•œ ì§„í–‰ë¥  í‘œì‹œ
                    if i % 10 == 0:
                        self.logger.info(f"ğŸ“Š íŠ¹ì§• ì¶”ì¶œ ì§„í–‰ë¥ : {i+1}/{len(scenes)}")
                
                feature_vector = self._extract_precision_features(scene.frame_path)
                if feature_vector is not None:
                    features.append(feature_vector)
                    valid_scenes.append(scene)
            except Exception as e:
                self.logger.warning(f"íŠ¹ì§• ì¶”ì¶œ ì‹¤íŒ¨: {scene.frame_path} - {str(e)}")
        
        if not features:
            return scenes[:self.target_scene_count]
        
        # 2. íŠ¹ì§• ì •ê·œí™”
        features_array = np.array(features)
        scaler = StandardScaler()
        features_normalized = scaler.fit_transform(features_array)
        
        # 3. ì •ë°€ë„ ë ˆë²¨ì— ë”°ë¥¸ ê±°ë¦¬ ê³„ì‚°
        distance_matrix = self._calculate_precision_distance(features_normalized)
        
        # 4. ì ì‘ì  í´ëŸ¬ìŠ¤í„°ë§ íŒŒë¼ë¯¸í„°
        eps = self._get_adaptive_clustering_eps(len(valid_scenes))
        min_samples = max(2, min(4, len(valid_scenes) // 15))  # ë” ì‘ì€ í´ëŸ¬ìŠ¤í„° í—ˆìš©
        
        clustering = DBSCAN(
            eps=eps,
            min_samples=min_samples,
            metric='precomputed'
        ).fit(distance_matrix)
        
        # 5. í´ëŸ¬ìŠ¤í„° ëŒ€í‘œ ì”¬ ì„ íƒ
        cluster_representatives = self._select_cluster_representatives(
            valid_scenes, features_normalized, clustering.labels_
        )
        
        # 6. ë…¸ì´ì¦ˆ í¬ì¸íŠ¸ ì²˜ë¦¬
        noise_scenes = [
            scene for i, scene in enumerate(valid_scenes) 
            if clustering.labels_[i] == -1
        ]
        
        # 7. ëª©í‘œ ê°œìˆ˜ì— ë§ì¶˜ ì”¬ ì„ íƒ ì „ëµ
        final_scenes = self._balance_scene_selection(
            cluster_representatives, noise_scenes, valid_scenes
        )
        
        # 8. ìµœì¢… ì”¬ ë¦¬ìŠ¤íŠ¸ (ì‹œê°„ìˆœ ì •ë ¬)
        final_scenes.sort(key=lambda s: s.timestamp)
        
        self.logger.info(
            f"ğŸ“Š ì •ë°€ë„ ë ˆë²¨ {self.precision_level} ê·¸ë£¹í™” ì™„ë£Œ: "
            f"{len(scenes)}ê°œ â†’ {len(final_scenes)}ê°œ "
            f"(ëª©í‘œ: {self.target_scene_count}ê°œ, í´ëŸ¬ìŠ¤í„°: {len(cluster_representatives)}, ë…ë¦½: {len(noise_scenes)})"
        )
        
        return final_scenes

    def _save_grouped_scenes(self, grouped_scenes: List[Scene], session_id: str) -> List[Scene]:
        """ê·¸ë£¹í™”ëœ ì”¬ë“¤ì„ ë³„ë„ ë””ë ‰í† ë¦¬ì— ì €ì¥"""
        grouped_dir = os.path.join(self.temp_dir, session_id, "grouped")
        os.makedirs(grouped_dir, exist_ok=True)
        
        updated_scenes = []
        for i, scene in enumerate(grouped_scenes):
            # ì›ë³¸ ì”¬ ì´ë¯¸ì§€ ê²½ë¡œ
            original_path = scene.frame_path
            
            # ê·¸ë£¹í™”ëœ ì”¬ ì´ë¯¸ì§€ ê²½ë¡œ
            grouped_filename = f"grouped_{i:04d}.jpg"
            grouped_path = os.path.join(grouped_dir, grouped_filename)
            
            # ì´ë¯¸ì§€ ë³µì‚¬
            try:
                import shutil
                shutil.copy2(original_path, grouped_path)
                
                # Scene ê°ì²´ ì—…ë°ì´íŠ¸ (grouped ê²½ë¡œ ì¶”ê°€)
                scene.grouped_path = grouped_path
                updated_scenes.append(scene)
                
                self.logger.debug(f"ê·¸ë£¹í™”ëœ ì”¬ ì €ì¥: {grouped_filename}")
            except Exception as e:
                self.logger.error(f"ê·¸ë£¹í™”ëœ ì”¬ ë³µì‚¬ ì‹¤íŒ¨: {str(e)}")
                updated_scenes.append(scene)
        
        return updated_scenes
        
    def _balance_scene_selection(self, cluster_reps: List[Scene], noise_scenes: List[Scene], all_scenes: List[Scene]) -> List[Scene]:
        """ëª©í‘œ ê°œìˆ˜ì— ë§ì¶˜ ê· í˜•ì¡íŒ ì”¬ ì„ íƒ"""
        current_count = len(cluster_reps) + len(noise_scenes)
        
        # ëª©í‘œ ê°œìˆ˜ë³´ë‹¤ ì ì€ ê²½ìš°
        if current_count < self.target_scene_count:
            needed = self.target_scene_count - current_count
            
            # ì‚¬ìš©ë˜ì§€ ì•Šì€ ì”¬ë“¤ì—ì„œ ì¶”ê°€ ì„ íƒ
            used_scenes = set(cluster_reps + noise_scenes)
            unused_scenes = [s for s in all_scenes if s not in used_scenes]
            
            if unused_scenes:
                # ì‹œê°„ì ìœ¼ë¡œ ê· ë“±í•˜ê²Œ ë¶„ì‚°ëœ ì”¬ ì¶”ê°€
                additional_scenes = self._select_time_distributed_scenes(unused_scenes, needed)
                result = cluster_reps + noise_scenes + additional_scenes
            else:
                result = cluster_reps + noise_scenes
                
        # ëª©í‘œ ê°œìˆ˜ë³´ë‹¤ ë§ì€ ê²½ìš°
        elif current_count > self.target_scene_count:
            # í´ëŸ¬ìŠ¤í„° ëŒ€í‘œ ì”¬ì„ ìš°ì„ ì ìœ¼ë¡œ ì„ íƒ
            if len(cluster_reps) >= self.target_scene_count:
                # í´ëŸ¬ìŠ¤í„° ëŒ€í‘œ ì”¬ë§Œìœ¼ë¡œ ëª©í‘œ ë‹¬ì„±
                result = self._select_diverse_scenes(cluster_reps)[:self.target_scene_count]
            else:
                # í´ëŸ¬ìŠ¤í„° ëŒ€í‘œ + ì¼ë¶€ ë…¸ì´ì¦ˆ ì”¬
                remaining_slots = self.target_scene_count - len(cluster_reps)
                selected_noise = self._select_diverse_scenes(noise_scenes)[:remaining_slots]
                result = cluster_reps + selected_noise
        else:
            # ì •í™•íˆ ëª©í‘œ ê°œìˆ˜
            result = cluster_reps + noise_scenes
        
        return result
    
    def _select_time_distributed_scenes(self, scenes: List[Scene], count: int) -> List[Scene]:
        """ì‹œê°„ì ìœ¼ë¡œ ê· ë“±í•˜ê²Œ ë¶„ì‚°ëœ ì”¬ ì„ íƒ"""
        if len(scenes) <= count:
            return scenes
        
        # ì‹œê°„ìˆœìœ¼ë¡œ ì •ë ¬
        sorted_scenes = sorted(scenes, key=lambda s: s.timestamp)
        
        # ê· ë“± ê°„ê²©ìœ¼ë¡œ ì„ íƒ
        interval = len(sorted_scenes) / count
        selected_indices = [int(i * interval) for i in range(count)]
        
        return [sorted_scenes[i] for i in selected_indices]
    
    def _get_adaptive_clustering_eps(self, scene_count: int) -> float:
        """ì”¬ ê°œìˆ˜ì™€ ì •ë°€ë„ ë ˆë²¨ì— ë”°ë¥¸ ì ì‘ì  eps ê°’"""
        base_eps = 1.0 - self.similarity_threshold
        
        # ì”¬ ê°œìˆ˜ì— ë”°ë¥¸ ì¡°ì •
        if scene_count > 30:
            count_factor = 0.8  # ë§ì€ ì”¬: ë” ì—„ê²©í•˜ê²Œ
        elif scene_count < 15:
            count_factor = 1.3  # ì ì€ ì”¬: ë” ê´€ëŒ€í•˜ê²Œ
        else:
            count_factor = 1.0
        
        # ì •ë°€ë„ ë ˆë²¨ì— ë”°ë¥¸ ì¡°ì •
        if self.precision_level <= 3:
            precision_factor = 1.5  # ë¹ ë¥¸ ì²˜ë¦¬: ë” ê´€ëŒ€í•œ ê·¸ë£¹í™”
        elif self.precision_level >= 8:
            precision_factor = 0.7  # ê³ ì •ë°€: ë” ì—„ê²©í•œ ê·¸ë£¹í™”
        else:
            precision_factor = 1.0
        
        return base_eps * count_factor * precision_factor
    
    def _extract_precision_features(self, image_path: str) -> Optional[np.ndarray]:
        """ì •ë°€ë„ ë ˆë²¨ì— ë”°ë¥¸ íŠ¹ì§• ì¶”ì¶œ"""
        try:
            # OpenCVë¡œ ì´ë¯¸ì§€ ì½ê¸°
            img_bgr = cv2.imread(image_path)
            if img_bgr is None:
                return None
            
            # ì •ë°€ë„ ë ˆë²¨ì— ë”°ë¥¸ ì´ë¯¸ì§€ í¬ê¸° ì¡°ì •
            if self.precision_level <= 3:
                # ë¹ ë¥¸ ì²˜ë¦¬ë¥¼ ìœ„í•´ í¬ê¸° ì¶•ì†Œ
                img_bgr = cv2.resize(img_bgr, (160, 120))
            elif self.precision_level >= 8:
                # ê³ ì •ë°€ì„ ìœ„í•´ í° í¬ê¸° ìœ ì§€
                img_bgr = cv2.resize(img_bgr, (320, 240))
            else:
                # ê¸°ë³¸ í¬ê¸°
                img_bgr = cv2.resize(img_bgr, (240, 180))
            
            img_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)
            img_hsv = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2HSV)
            img_gray = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2GRAY)
            
            features = []
            
            # í™œì„±í™”ëœ íŠ¹ì§•ë§Œ ì¶”ì¶œ
            if 'color_hist' in self.active_features:
                features.extend(self._extract_color_histogram(img_hsv))
            
            if 'edge_density' in self.active_features:
                features.append(self._extract_edge_density(img_gray))
            
            if 'texture' in self.active_features:
                features.extend(self._extract_lbp_features(img_gray))
            
            if 'spatial_color' in self.active_features:
                features.extend(self._extract_spatial_color(img_hsv))
            
            if 'perceptual_hash' in self.active_features:
                features.extend(self._extract_perceptual_hash(img_rgb))
            
            if 'brightness' in self.active_features:
                features.extend(self._extract_brightness_stats(img_hsv))
            
            if 'contrast' in self.active_features:
                features.append(self._extract_contrast(img_gray))
            
            if 'color_diversity' in self.active_features:
                features.append(self._extract_color_diversity(img_rgb))
            
            return np.array(features) if features else None
            
        except Exception as e:
            self.logger.error(f"ì •ë°€ë„ íŠ¹ì§• ì¶”ì¶œ ì˜¤ë¥˜: {str(e)}")
            return None
    
    def _extract_color_histogram(self, img_hsv: np.ndarray) -> List[float]:
        """ìƒ‰ìƒ íˆìŠ¤í† ê·¸ë¨ ì¶”ì¶œ"""
        # ì •ë°€ë„ ë ˆë²¨ì— ë”°ë¥¸ íˆìŠ¤í† ê·¸ë¨ ë¹ˆ ìˆ˜ ì¡°ì •
        bins = 16 if self.precision_level <= 3 else 32 if self.precision_level <= 7 else 64
        
        hist_h = cv2.calcHist([img_hsv], [0], None, [bins], [0, 180])
        hist_s = cv2.calcHist([img_hsv], [1], None, [bins], [0, 256])
        hist_v = cv2.calcHist([img_hsv], [2], None, [bins], [0, 256])
        
        color_hist = np.concatenate([hist_h, hist_s, hist_v]).flatten()
        color_hist = color_hist / (color_hist.sum() + 1e-7)
        
        return color_hist.tolist()
    
    def _extract_edge_density(self, img_gray: np.ndarray) -> float:
        """ì—£ì§€ ë°€ë„ ì¶”ì¶œ"""
        # ì •ë°€ë„ ë ˆë²¨ì— ë”°ë¥¸ Canny ì„ê³„ê°’ ì¡°ì •
        if self.precision_level <= 3:
            edges = cv2.Canny(img_gray, 100, 200)
        elif self.precision_level >= 8:
            edges = cv2.Canny(img_gray, 30, 100)
        else:
            edges = cv2.Canny(img_gray, 50, 150)
        
        return float(np.sum(edges > 0) / edges.size)
    
    def _extract_lbp_features(self, gray_img: np.ndarray, num_points: int = 8, radius: int = 1) -> List[float]:
        """Local Binary Patterns íŠ¹ì§• ì¶”ì¶œ"""
        h, w = gray_img.shape
        lbp = np.zeros_like(gray_img)
        
        # ì •ë°€ë„ ë ˆë²¨ì— ë”°ë¥¸ LBP íŒŒë¼ë¯¸í„° ì¡°ì •
        if self.precision_level <= 3:
            num_points = 6  # ë¹ ë¥¸ ì²˜ë¦¬
        elif self.precision_level >= 8:
            num_points = 12  # ì •ë°€í•œ ì²˜ë¦¬
        
        for i in range(radius, h - radius):
            for j in range(radius, w - radius):
                center = gray_img[i, j]
                binary_code = 0
                
                for k in range(num_points):
                    angle = 2 * np.pi * k / num_points
                    x = int(round(i + radius * np.cos(angle)))
                    y = int(round(j + radius * np.sin(angle)))
                    
                    if 0 <= x < h and 0 <= y < w and gray_img[x, y] >= center:
                        binary_code |= (1 << k)
                
                lbp[i, j] = binary_code
        
        # LBP íˆìŠ¤í† ê·¸ë¨
        bins = 16 if self.precision_level <= 5 else 32
        hist, _ = np.histogram(lbp, bins=bins, range=(0, 256))
        hist = hist.astype(float) / (hist.sum() + 1e-7)
        
        return hist.tolist()
    
    def _extract_spatial_color(self, img_hsv: np.ndarray) -> List[float]:
        """ê³µê°„ë³„ ìƒ‰ìƒ ë¶„í¬ ì¶”ì¶œ"""
        h, w = img_hsv.shape[:2]
        
        # ì •ë°€ë„ ë ˆë²¨ì— ë”°ë¥¸ ê·¸ë¦¬ë“œ í¬ê¸° ì¡°ì •
        if self.precision_level <= 3:
            grid_size = 2
        elif self.precision_level <= 6:
            grid_size = 3
        else:
            grid_size = 4
        
        spatial_features = []
        
        for i in range(grid_size):
            for j in range(grid_size):
                y1 = i * h // grid_size
                y2 = (i + 1) * h // grid_size
                x1 = j * w // grid_size
                x2 = (j + 1) * w // grid_size
                
                region = img_hsv[y1:y2, x1:x2]
                bins = 8 if self.precision_level <= 5 else 16
                region_hist = cv2.calcHist([region], [0], None, [bins], [0, 180])
                region_hist = region_hist.flatten() / (region_hist.sum() + 1e-7)
                spatial_features.extend(region_hist)
        
        return spatial_features
    
    def _extract_perceptual_hash(self, img_rgb: np.ndarray) -> List[float]:
        """ì§€ê°ì  í•´ì‹œ ì¶”ì¶œ"""
        pil_img = Image.fromarray(img_rgb)
        
        # ì •ë°€ë„ ë ˆë²¨ì— ë”°ë¥¸ í•´ì‹œ í¬ê¸° ì¡°ì •
        hash_size = 4 if self.precision_level <= 3 else 8 if self.precision_level <= 7 else 16
        
        phash = imagehash.phash(pil_img, hash_size=hash_size)
        dhash = imagehash.dhash(pil_img, hash_size=hash_size)
        
        # í•´ì‹œë¥¼ ë°”ì´ë„ˆë¦¬ ë°°ì—´ë¡œ ë³€í™˜
        hash_features = []
        
        for hash_obj in [phash, dhash]:
            for char in str(hash_obj):
                if char in '0123456789abcdef':
                    binary = format(int(char, 16), '04b')
                    hash_features.extend([float(b) for b in binary])
        
        return hash_features
    
    def _extract_brightness_stats(self, img_hsv: np.ndarray) -> List[float]:
        """ë°ê¸° í†µê³„ ì¶”ì¶œ"""
        brightness = img_hsv[:, :, 2]
        mean_brightness = np.mean(brightness) / 255.0
        std_brightness = np.std(brightness) / 255.0
        
        if self.precision_level >= 7:
            # ê³ ì •ë°€ë„ì—ì„œëŠ” ì¶”ê°€ í†µê³„ í¬í•¨
            min_brightness = np.min(brightness) / 255.0
            max_brightness = np.max(brightness) / 255.0
            return [mean_brightness, std_brightness, min_brightness, max_brightness]
        
        return [mean_brightness, std_brightness]
    
    def _extract_contrast(self, img_gray: np.ndarray) -> float:
        """ëŒ€ë¹„ ì¶”ì¶œ"""
        return float(img_gray.std() / 255.0)
    
    def _extract_color_diversity(self, img_rgb: np.ndarray) -> float:
        """ìƒ‰ìƒ ë‹¤ì–‘ì„± ì¶”ì¶œ"""
        h, w = img_rgb.shape[:2]
        
        # ì •ë°€ë„ ë ˆë²¨ì— ë”°ë¥¸ ìƒ˜í”Œë§ ì¡°ì •
        if self.precision_level <= 3:
            # ë¹ ë¥¸ ì²˜ë¦¬ë¥¼ ìœ„í•´ ìƒ˜í”Œë§
            sample_rate = 4
            sampled = img_rgb[::sample_rate, ::sample_rate].reshape(-1, 3)
        else:
            sampled = img_rgb.reshape(-1, 3)
        
        unique_colors = len(np.unique(sampled, axis=0))
        total_pixels = len(sampled)
        
        return float(unique_colors / total_pixels)
    
    def _calculate_precision_distance(self, features: np.ndarray) -> np.ndarray:
        """ì •ë°€ë„ ë ˆë²¨ì— ë”°ë¥¸ ê°€ì¤‘ì¹˜ ê¸°ë°˜ ê±°ë¦¬ í–‰ë ¬ ê³„ì‚°"""
        n_samples = features.shape[0]
        distance_matrix = np.zeros((n_samples, n_samples))
        
        # íŠ¹ì§•ë³„ ì¸ë±ìŠ¤ ë²”ìœ„ ë™ì  ê³„ì‚°
        idx = 0
        feature_ranges = {}
        
        for feat_name in self.active_features:
            if feat_name == 'color_hist':
                size = self._get_color_hist_size()
                feature_ranges[feat_name] = (idx, idx + size)
                idx += size
            elif feat_name == 'edge_density':
                feature_ranges[feat_name] = (idx, idx + 1)
                idx += 1
            elif feat_name == 'texture':
                size = 16 if self.precision_level <= 5 else 32
                feature_ranges[feat_name] = (idx, idx + size)
                idx += size
            elif feat_name == 'spatial_color':
                grid_size = 2 if self.precision_level <= 3 else 3 if self.precision_level <= 6 else 4
                bins = 8 if self.precision_level <= 5 else 16
                size = grid_size * grid_size * bins
                feature_ranges[feat_name] = (idx, idx + size)
                idx += size
            elif feat_name == 'perceptual_hash':
                hash_size = 4 if self.precision_level <= 3 else 8 if self.precision_level <= 7 else 16
                size = hash_size * hash_size * 2 * 4  # phash + dhash, 4ë¹„íŠ¸ per hex char
                feature_ranges[feat_name] = (idx, idx + size)
                idx += size
            elif feat_name == 'brightness':
                size = 4 if self.precision_level >= 7 else 2
                feature_ranges[feat_name] = (idx, idx + size)
                idx += size
            elif feat_name == 'contrast':
                feature_ranges[feat_name] = (idx, idx + 1)
                idx += 1
            elif feat_name == 'color_diversity':
                feature_ranges[feat_name] = (idx, idx + 1)
                idx += 1
        
        # ê° íŠ¹ì§•ë³„ë¡œ ê±°ë¦¬ ê³„ì‚° í›„ ê°€ì¤‘ì¹˜ ì ìš©
        for feat_name, (start, end) in feature_ranges.items():
            if start < features.shape[1] and feat_name in self.feature_weights:
                weight = self.feature_weights[feat_name]
                actual_end = min(end, features.shape[1])
                
                if start < actual_end:
                    feat_distances = cdist(
                        features[:, start:actual_end], 
                        features[:, start:actual_end], 
                        metric='euclidean'
                    )
                    # ì •ê·œí™”
                    if feat_distances.max() > 0:
                        feat_distances = feat_distances / feat_distances.max()
                    distance_matrix += weight * feat_distances
        
        return distance_matrix
    
    def _get_color_hist_size(self) -> int:
        """ì •ë°€ë„ ë ˆë²¨ì— ë”°ë¥¸ ìƒ‰ìƒ íˆìŠ¤í† ê·¸ë¨ í¬ê¸° ë°˜í™˜"""
        bins = 16 if self.precision_level <= 3 else 32 if self.precision_level <= 7 else 64
        return bins * 3  # H, S, V
    
    def _select_cluster_representatives(
        self, 
        scenes: List[Scene], 
        features: np.ndarray, 
        labels: np.ndarray
    ) -> List[Scene]:
        """ê° í´ëŸ¬ìŠ¤í„°ì—ì„œ ê°€ì¥ ëŒ€í‘œì ì¸ ì”¬ ì„ íƒ"""
        representatives = []
        unique_labels = set(labels)
        unique_labels.discard(-1)  # ë…¸ì´ì¦ˆ ì œì™¸
        
        for label in unique_labels:
            # í´ëŸ¬ìŠ¤í„°ì— ì†í•œ ì”¬ë“¤ì˜ ì¸ë±ìŠ¤
            cluster_indices = np.where(labels == label)[0]
            
            if len(cluster_indices) == 1:
                representatives.append(scenes[cluster_indices[0]])
            else:
                # ì •ë°€ë„ ë ˆë²¨ì— ë”°ë¥¸ ëŒ€í‘œ ì”¬ ì„ íƒ ë°©ì‹
                if self.precision_level <= 3:
                    # ë¹ ë¥¸ ì²˜ë¦¬: ì²« ë²ˆì§¸ ì”¬ ì„ íƒ
                    representatives.append(scenes[cluster_indices[0]])
                else:
                    # í´ëŸ¬ìŠ¤í„° ì¤‘ì‹¬ê³¼ ê°€ì¥ ê°€ê¹Œìš´ ì”¬ ì„ íƒ
                    cluster_features = features[cluster_indices]
                    center = np.mean(cluster_features, axis=0)
                    distances = np.sum((cluster_features - center) ** 2, axis=1)
                    closest_idx = cluster_indices[np.argmin(distances)]
                    representatives.append(scenes[closest_idx])
        
        return representatives
    
    def _select_diverse_scenes(self, scenes: List[Scene]) -> List[Scene]:
        """ë‹¤ì–‘ì„±ì„ ê³ ë ¤í•œ ì”¬ ì„ íƒ"""
        if len(scenes) <= self.target_scene_count:
            return scenes
        
        # ì •ë°€ë„ ë ˆë²¨ì— ë”°ë¥¸ ì„ íƒ ë°©ì‹
        if self.precision_level <= 3:
            # ë¹ ë¥¸ ì²˜ë¦¬: ê· ë“± ê°„ê²© ìƒ˜í”Œë§
            interval = len(scenes) / self.target_scene_count
            selected_indices = [int(i * interval) for i in range(self.target_scene_count)]
            return [scenes[i] for i in selected_indices]
        else:
            # ì •ë°€í•œ ì²˜ë¦¬: ì‹œê°„ì  ë¶„ì‚°ì„ ê³ ë ¤í•œ ì„ íƒ
            if len(scenes) == 0:
                return []
            
            selected = [scenes[0]]  # ì²« ë²ˆì§¸ ì”¬ í¬í•¨
            remaining = scenes[1:]
            
            while len(selected) < self.target_scene_count and remaining:
                # ì„ íƒëœ ì”¬ë“¤ê³¼ ì‹œê°„ì ìœ¼ë¡œ ê°€ì¥ ë©€ë¦¬ ë–¨ì–´ì§„ ì”¬ ì„ íƒ
                max_min_distance = -1
                best_scene = None
                best_idx = -1
                
                for i, candidate in enumerate(remaining):
                    min_distance = min(
                        abs(candidate.timestamp - selected_scene.timestamp)
                        for selected_scene in selected
                    )
                    
                    if min_distance > max_min_distance:
                        max_min_distance = min_distance
                        best_scene = candidate
                        best_idx = i
                
                if best_scene:
                    selected.append(best_scene)
                    remaining.pop(best_idx)
                else:
                    break
            
            return sorted(selected, key=lambda s: s.timestamp)
    
    def _get_video_info(self, video_path: str) -> Dict[str, Any]:
        """ë¹„ë””ì˜¤ ì •ë³´ ì¶”ì¶œ"""
        cmd = [
            'ffprobe',
            '-v', 'quiet',
            '-print_format', 'json',
            '-show_format',
            '-show_streams',
            video_path
        ]
        
        try:
            result = subprocess.run(cmd, capture_output=True, text=True, check=True)
            info = json.loads(result.stdout)
            
            # ë¹„ë””ì˜¤ ìŠ¤íŠ¸ë¦¼ ì°¾ê¸°
            video_stream = None
            for stream in info.get('streams', []):
                if stream.get('codec_type') == 'video':
                    video_stream = stream
                    break
            
            return {
                'duration': float(info.get('format', {}).get('duration', 0)),
                'width': int(video_stream.get('width', 0)) if video_stream else 0,
                'height': int(video_stream.get('height', 0)) if video_stream else 0,
                'fps': eval(video_stream.get('r_frame_rate', '0/1')) if video_stream else 0
            }
            
        except Exception as e:
            self.logger.error(f"ë¹„ë””ì˜¤ ì •ë³´ ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}")
            return {}